<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Supercharge your VLM with Bag-of-Concept Graph to mitigate hallucinations!">
  <meta property="og:title" content="GitHub"/>
  <meta property="og:description" content="GitHub is a hosting platform for open source and private software projects, because it only supports Git as the only version library format for hosting, so the name GitHub."/>
  <meta property="og:url" content="https://ankortn.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="vision language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations</title>
  <link rel="icon" type="image/x-icon" href="https://cdn-icons-png.flaticon.com/512/954/954591.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate Hallucinations</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                  <span class="author-block">
                    <a href="ztyang196@gmail.com" target="_blank">Zhantao Yang</a><sup>1,2,*;</sup>,
                  </span>
                  <span class="author-block">
                    <a href="ruilifengustc@gmail.com" target="_blank">Ruili Feng</a><sup>2,*;</sup>,
                  </span>
                  <span class="author-block">
                      Keyu Yan<sup>2</sup>,
                  </span>
                  <span class="author-block">
                      Huangji Wang<sup>1</sup>,
                  </span>
                  <span class="author-block">
                      Zhicai Wang<sup>2,3</sup>,
                  </span>
                  <span class="author-block">
                      Shangwen Zhu<sup>1</sup>,
                  </span>
                  <span class="author-block">
                      Han Zhang<sup>1,2</sup>,
                  </span>
                  <span class="author-block">
                      Jie Xiao<sup>2,3</sup>,
                  </span>
                  <span class="author-block">
                      Pingyu Wu<sup>2,3</sup>,
                  </span>
                  <span class="author-block">
                      Kai Zhu<sup>2</sup>,
                  </span>
                  <span class="author-block">
                      Jixuan Chen<sup>2</sup>,
                  </span>
                  <span class="author-block">
                      Chen-Wei Xie<sup>2</sup>,
                  </span>
                  <span class="author-block">
                      Chaojie Mao<sup>2</sup>,
                  </span>
                  <span class="author-block">
                      Yue Yang<sup>4</sup>,
                  </span>
                  <span class="author-block">
                      Hongyang Zhang<sup>5</sup>,
                  </span>
                  <span class="author-block">
                      Yu Liu<sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="chengfan@sjtu.edu.cn" target="_blank">Fan Cheng</a><sup>1,&#8224;</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>Shanghai Jiao Tong University, <sup>2</sup>Alibaba group<br> <sup>3</sup>University of Science and Technology of China<br> <sup>4</sup>University of Pennsylvania, <sup>5</sup>University of Waterloo
                      <!-- <br> -->
                      <!-- Conferance name and year -->
                    </span>
                    <span class="eql-cntrb"><small><br>&#8224;Corresponding author, <sup>*</sup>Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/EMNLP2024_Graph_Caption.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="PleaseEnterHere" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="far fa-images"></i>
                      </span>
                      <span>Demo</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Dataset link -->
              <span class="link-block">
                <a href="Which_dataset" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset</span>
                </a>
              </span>

              <!-- Model link -->
              <span class="link-block">
                <a href="readme" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-share-square"></i>
                  </span>
                  <span>Model</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section" style="background-color:#efeff081">
  <div class="container is-max-desktop" id="gradio">
    <iframe src="https://www.example.com" width="100%" height="600" frameborder="0" style="border:0; overflow:hidden;"></iframe>
  </div>
</section>
<!-- <section class="section"  style="background-color:#efeff081">
  <div class="container is-max-desktop" id="gradio">
    <gradio-app src="https://llava.hliu.cc"></gradio-app>
  </div>
</section> -->



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper presents <strong>Ba</strong>g-of-<strong>Con</strong>cept Graph (<strong>BACON</strong>) to gift models with limited linguistic abilities to taste the privilege of Vision Language Models (VLMs) and reduce hallucinations in the downstream tasks such as detection, visual question answering (VQA), and image generation. Since the visual scenes in physical worlds are structured with complex relations between objects, BACON breaks down annotations into basic minimum elements and presents them in a graph structure. Element-wise style enables easy understanding, and structural composition liberates difficult locating. Careful prompt design births the BACON captions with the help of publicly available VLMs and segmentation methods. In this way, we gather a dataset with 100K annotated images, which endow VLMs with remarkable capabilities, such as accurately generating BACON, transforming prompts into BACON format, envisioning scenarios in the style of BACON, and dynamically modifying elements within BACON through interactive dialogue and more. Wide representative experiments, including detection, VQA, and image generation tasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel in their current cutting-edge solutions.
          </p>
          <!-- <p>
              Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks in the language domain, but the idea is less explored in the multimodal field.
              <ol type="1">
                <li><b>Multimodal Instruct Data</b>. <span style="font-size: 95%;">We present the first attempt to use <a href="https://openai.com/research/gpt-4">language-only GPT-4</a> to generate multimodal language-image instruction-following data. </span></li>
                <li><b>LLaVA Model</b>. <span style="font-size: 95%;">We introduce <it><b>LLaVA</b> (<b>L</b>arge <b>L</b>anguage-<b>a</b>nd-<b>V</b>ision <b>A</b>ssistant)</it>, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.</li>
                <li><b>Performance</b>. <span style="font-size: 95%;">Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset.
                  When fine-tuned on <a href="https://scienceqa.github.io/">Science QA</a>, the synergy of LLaVA and GPT-4  achieves a new state-of-the-art accuracy of 92.53%.</li>
                <li><b>Open-source</b>. <span style="font-size: 95%;">We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.</li>
              </ol>  
           </p> -->
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <br>
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/1447/PNG/512/32381bacon_98873.png"> BACON: Bag-of-Concept Graph</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>         
          <ul type="1">
            <li><b>Graph Construction</b>. </li>
            <ul type="1">
              <li> <b>Deconstructing annotations</b>: BACON decomposes the annotations of VLMs into basic elements and then combining them according to a specific structure.
              <li> <b>BACON-Captioner</b>: Fine-tune a 13B LLaVA model on the BACON dataset to serve as a specialized captioner.</li>
              <li> <b>Graph Structure</b>: BACON deconstructs the <b>image annotations</b> into <b>a specific graph structure</b>, including <b>overall description, object list, and relationships</b>.</li>
            </ul>  
            <div style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/first_image_3.png">     
            </div>
            <li><b>Graph Grounding</b>. <span style="font-size: 95%;"></li>
              <ul type="1">
                <li><b>1. Get BACON:</b> Derive BACON by BACON-Captioner;</li>
                <li><b>2. Graph Grounding:</b> Give each object corresponding region;</li>
                <li><b>3. Distill Grounding:</b> Using LLaVA to discard blatant incorrect regions;</li>
                <li><b>4. Match Best</b>: Match the most similar segmentation by CLIP.</li>
              </ul>
              <div style="text-align: center;">
                <img id="teaser" width="100%" src="static/images/grounding_dino.png">     
              </div>
            <li><b>Root Words and Detected Catagories</b> in BACON
            </li>
            <ul type="1">
              <li><b>Substitutability:</b> BACON-Captioner can effectively take over from GPT-4V in generating BACON from images.</li>
            </ul>
            <div style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/statistic_2.png">     
            </div>
          </ul>  
        </p>
      </div>
    </div>
  </div>
</section>





<section class="hero is-small is-light">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <br>
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/3276/PNG/512/egg_bacon_dish_plate_food_icon_207982.png"> BACON Dataset</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p>
          To the best of our knowledge, this is the <b>first</b> method designed for <b>open-vocabulary tasks</b>.
          <!-- <centering>
            <div style="text-align: center;">
              <img id="teaser" width="70%" src="static/images/first_image_1.png">     
            </div>
          </centering> -->    
          <ul type="1">
            <li><b>Training Set</b>. <span style="font-size: 95%;"></li>
            <ul type="1">
              <li> <b>Large Scale</b>: Refined <b>100k</b> BACON-image pairs. </li>
              <li> <b>Distill Process</b>: Graph construction + graph grounding. (As is shown in the previous section)</li>
            </ul>  
            <div style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/train_pipeline.png">     
            </div>
            <li><b>Test Benchmark</b>.</li> 
              <ul type="1">
                <li> <b>Large Scale</b>: <b>4k</b> images, <b>40k</b> objects, and <b>200k</b> relationships. </li>
                <li> <b>Distill Process</b>: Generate BACON-style <b>(1)(2)(3)</b> indivisually.</li>
                <li><b>Human Fine-tune</b>: Human annotation is required to correct and verify the outputs.</li>
                <li><b>Detailed overview</b>: </li>
                <ul type="1">
                  <li><b>1. Segmentation by SAM</b>.</li>
                  <li><b>2. Get Lists of Objects by VLMs</b>.</li>
                  <li><b>3. Overall Description Annotations by VLMs.</b></li>
                  <li><b>4. Identify relationships by VLMs</b>. </li>
                </ul>
              </ul> 
              <div style="text-align: center;">
                <img id="teaser" width="100%" src="static/images/test_pipeline.png">     
              </div>
          </ul>  
        </p>
      </div>
    </div>
  </div>
</section>



<section class="hero is-small">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <br>
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/3780/PNG/512/factory_robot_manufacturing_industry_robotics_machine_robotic_arm_automation_industrial_production_icon_231898.png"> BACON Capioner</h2>
    </div>
  </div>

<div class="container is-max-desktop">
  <div class="columns is-centered">
    <div class="column is-full-width">
      <div class="content has-text-justified"> 
        <p> 
          <ul type="1">
            <li><b>Image Generation Based on Captioner</b></li>
            <br>
            <div style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/image_generation.png">     
            </div>
            <br>
            <li><b>Interactively Modify BACON with Captioner</b></li> 
            <br>
            <div style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/edit.png">     
            </div>
            <br>
            <li><b>Distinctive Style of BACON with Prompts</b></li> 
            <br>
            <div style="text-align: center;">
              <img id="teaser" width="100%" src="static/images/recaption_3.png">     
            </div>
          </ul>  
        </p>
      </div>
    </div>
  </div>

</section>


<section class="hero is-small is-light">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <br>
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/3310/PNG/512/math_book_school_study_icon_209279.png"> Experiments</h2>
    </div>
  </div>
<div class="container is-max-desktop">


  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"><img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/2178/PNG/512/household_chores_task_washing_machine_clothes_appliance_icon_133381.png"> Downstream tasks benefiting from BACON</h2>
      <div class="content has-text-justified"> 
        The adaptable nature of BACON's structure enhances models' comprehension of complex text and empowers them to undertake tasks previously beyond their reach. We conduct evaluations across five downstream tasks, including object detection, point question answering (PointQA), Pointing question answering (PointingQA), scene graph generation (SGG), and image generation.<br><br>
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="100%" src="static/images/first_image_2.png">     
        </div>
        <br>
        <b>Schematic diagram of multiple exemplary downstream tasks can benefit from  BACON</b>. <br>Specifically, BACON can:
        <ul type="1">
          <li>enable VLMs to carry out the point question answering task previously beyond their scope;</li>
          <li>assist text-to-image generative models such as SDXL in creating intricate images with higher precision as demanded by prompts;</li>
          <li>execute open-vocabulary scene graph generation tasks that were not feasible for other VLMs.</li>
        </ul>
      </centering>
      </div>
    </div>
  </div>


  <!-- Grounedtext2img. -->
  <div class="columns is-centered">
    <div class="column is-full-width">
      <h2 class="title is-4"> <img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/1146/PNG/512/1486485582-311arrow-film-movie-play-player-start-video_81177.png">BACON on video captioning</h2>
      <div class="content has-text-justified"> 
      While BACON is primarily developed for image data, it can be extended to create structured captions for videos with the help of additional techniques that address the temporal dimension of video content.
      <br><br>
      <centering>
        <div style="text-align: center;">
          <img id="teaser" width="100%" src="static/images/video_caption_example_result_3.png">     
        </div>
      </centering>
      <br>
      This is <b>an example of BACON on video captioning</b>, which includes three components: an overall description, an object list, and their relationships, each dynamically evolving over time. With respect to a prior frame, updates are color-coded: new elements in green, removed in red, altered in gold, and persistent ones in black. BACON thus adeptly captures the temporal changes and salient details of each video frame, while its structured nature potentially aids in downstream model comprehension.
    </div>
    </div>
  </div>
  <!-- Grounedtext2img. -->
  
  <div class="column is-full-width">
    <h2 class="title is-4"> <img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/561/PNG/512/seo-performance-marketing-graphic_icon-icons.com_53826.png">Performance</h2>
  </div>
</div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/pointing task.png" style="width: 100%;" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>Quantitative comparison</b> on (Left) PointQA and (Right) PointingQA between BACON and baselines.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/precision_recall.png" style="width: 100%;" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          <b>Win rate of pairwise comparisons</b> between BACON-Captioner and other VLM-based captioners.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/comparison.png" style="width: 100%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Comparison of open-vocabulary object detection</b> among BACON, Grounding DINO, openvocabulary object detection models, and grounding caption models on BACON benchmark. GD represents Grounding DINO. We have calculated error bars for models that exhibit randomness.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/VG.png" style="width: 100%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Comparison on open-vocabulary scene graph generation task</b> between BACON and multiple baselines on VG dataset and BACON benchmark. The number of correct predictions is used as the metric.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/VQA.png" style="width: 100%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Quantitative comparison of VQA task</b> between BACON and multiple VLM-based baselines, where the input image of the QA model is replaced by its caption to evaluate the performance of the captioner. The metric is the accuracy of answering questions.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/accuracy.png" style="width: 100%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Accuracy in depicting objects (Ao) and relationships (Ar)</b>  in images generated from text prompts, as evaluated by human. We compare SDXL enhanced by BACON with SDXL and DALL-E 3.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/manual.png" style="width: 100%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Precision and recall score calculated by manual annotation</b> between BACON-Captioner and other VLM-based captioners.
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/plantask.png" style="width: 100%;" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Comparison of plan task</b> between BACON and LayoutGPT on both MSCOCO and BACON benchmark.
      </h2>
    </div>
    <div class="item" style="text-align: center;">
      <!-- Your image here -->
        <img style="width: 50%;" src="static/images/grounding_dino_ablation.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        <b>Ablation study</b> of method in graph grounding, exploring the improvement of introducing CLIP and LLaVA, where the experiment is conducted on BACON benchmark.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<section class="hero is-small is-light">
  <!-- Results. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <br>
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/1137/PNG/512/1486394979-11-data-visualization_80549.png"> Visualization on Video Captioning</h2>
    </div>
  </div>
  <!-- </div> -->
  <!--/ Results. -->    
<div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/track_res_output.mp4"
        type="video/mp4">
      </video>
      <h2>
        While BACON is primarily developed for image data, it can be extended to create structured captions for videos with the help of additional techniques that address the temporal dimension of video content.
      </h2>
    </div>
  </div>
</section>


<section class="hero is-small">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3"><img id="painting_icon" width="3%" src="https://cdn.icon-icons.com/icons2/4215/PNG/512/analysis_research_example_case_study_icon_262980.png"> Examples about BACON</h2>
    </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
       <h2 class="title is-4">Image Generation</h2>
    </div>
    </div>  

  <div class="columns is-centered has-text-centered">
  <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
    <img id="teaser" width="80%" src="static/images/image_generation_appendix_2.png">
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
       <h2 class="title is-4">Video Captioning</h2>
    </div>
    </div>  

  <div class="columns is-centered has-text-centered">
  <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
    <img id="teaser" width="80%" src="static/images/video_caption_example_result_1.png">
  </div>
  </div>
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
      <img id="teaser" width="80%" src="static/images/video_caption_example_result_2.png">
    </div>
    </div>



  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
       <h2 class="title is-4">Examples of BACON in String Format Obtained by GPT-4V</h2>
    </div>
    </div>  

  <div class="columns is-centered has-text-centered">
  <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
    <img id="teaser" width="45%" src="static/images/string_example.png">
  </div>
  </div>

  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
       <h2 class="title is-4">Instruction for GPT-4V to Obtain BACON</h2>
    </div>
    </div>  

  <div class="columns is-centered has-text-centered">
  <div class="column is-six-fifths" style="display: flex; align-items: flex-start; justify-content: center;">
    <img id="teaser" width="45%" src="static/images/instruction.png">
  </div>
  </div>

</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <br>
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{abc,
          author      = {def},
          title       = {ghi},
          booktitle   = {jkl},
          year        = {nmo}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <br>
    <h2 class="title">Acknowledgement</h2>
    <p>
      This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
             <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
</section>

<!-- 
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
             <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
</html>